+++
title = "MATH246H"
author = ["ksam"]
date = 2022-01-26
lastmod = 2022-02-24T12:52:07-05:00
draft = false
slug = "MATH246H"
description = "Notes from UMD's MATH246H class"
math = true
+++

## Definitions and Terminology of Differential Equations {#definitions-and-terminology-of-differential-equations}


### Definition of DEs {#definition-of-des}

Some combination of one or more unknown functions with respect to one or more independent variable and those variables


### Ordinary DEs {#ordinary-des}

A DE where there is only one unknown variable. A DE that is not ordinary is a partial DE


### Order of DEs {#order-of-des}

The highest derivative that appears in the DE.


### Linearity {#linearity}

A DE is linear if there is no function composition on the unknown functions or their derivatives.


### Systems of DEs {#systems-of-des}

-   It is a system of ODEs if there is only one unknown variable. A system that does not satisfy this is a system of PDEs.
-   The order of a system is the highest order of all of its equations.
-   A system is linear if every equation in it is linear.


## Linear Equations {#linear-equations}

-   DEs that are [linear](#linearity)


### Linear Normal Form {#linear-normal-form}

\\[\frac{dy}{dt} + \frac{q(t)}{p(t)}y = \frac{r(t)}{p(t)}\\]
From now on, \\(a(t) = \frac{q(t)}{p(t)}\\) and \\(f(t) = \frac{r(t)}{p(t)}\\)


#### Homogeneous {#homogeneous}

\\[\frac{dy}{dt} + a(t)y = 0\\]
By analyzing the equation, we see that this means that \\(y\\) is equal to its derivative. We know that \\(e^x\\) operates as such and we can conclude that the general solution to a Homogeneous Linear DE is:
\\[y = Ce^{-A(t)}\\]
where \\(A(t) = \int a(t)dt\\).


#### Inhomogeneous {#inhomogeneous}

\\[\frac{dy}{dt} + a(t)y = f(t)\\]
This isn't quite as nice as the [homogeneous](#homogeneous) version, but we can still solve it.

<!--list-separator-->

-  Integrating Factor

    Multiply both sides by \\(e^{A(t)}\\).
    \\[e^{A(t)}\frac{dy}{dt} + a(t)e^{A(t)}y = e^{A(t)}f(t)\\]
    The left side is the derivative of \\(e^{A(t)}y\\)
    \\[\frac{d}{dt}(e^{A(t)}y) = e^{A(t)}f(t)\\]
    This is integrating factor form. Now we can integrate both sides and solve for \\(y\\) to get the solution.

<!--list-separator-->

-  General Solution

    \\[y = e^{-A(t)}\int e^{A(t)}f(t)dt + Ce^{-A(t)}\\]


## Separable Equations {#separable-equations}

Equations that can be written in the form:
\\[\frac{dy}{dt} = g(y)f(t)\\]
We simply multiply both sides by \\(dt\\) and divide by \\(g(y)\\) and integrate both sides to solve. The general solution is:
\\[\int \frac{1}{g(y)}dy = \int f(t)dt\\]
Assuming both of these integrals are computable,
\\[G(y) = F(t) + C\\]
Assuming that \\(G(y)\\) has an inverse, \\(G^{-1}(y)\\),
\\[y = G^{-1}(F(t) + C)\\]


### Autonomous Equations {#autonomous-equations}

An equation of the form:
\\[\frac{dy}{dt} = g(y)\\]


## General Theory {#general-theory}


### Interior {#interior}

-   If you can draw a rectangle around the point that is entirely container in the set


### Existence and Uniqueness Theorem {#existence-and-uniqueness-theorem}

For the problem \\(\frac{dy}{dt} = f(t,y)\\), \\(y(t\_1) = y\_1\\)

If \\(f(t,y)\\) is a function defined over a set \\(S\\) in the $ty$-plane such that

1.  \\(f\\) is continuous over \\(S\\)
2.  \\(f\\) is differentiable with respect to \\(y\\) over \\(S\\)
3.  \\(\partial\_yf\\) is continuous over \\(S\\)

Then for every initial time \\(t\_1\\) and initial value \\(y\_1\\) such that \\((t\_1,y\_1)\\) is in the interior of \\(S\\), there is a unique solution to the initial value problem that is defined over the largest time interval that contains the initial time and all points are in \\(S\\) and is smooth in that interval.

Basically, this means that given some differential equation, if it and its first-derivative of \\(y\\) are continuous, then there is a unique solution to the differential equation.

Because there is a unique solution to every one, two solutions can never cross. If they do, that point could be used as an initial point and produce non-unique solutions, which is not allowed.


## Graphical Methods {#graphical-methods}


### Phase Portraits for Autonomous Equations {#phase-portraits-for-autonomous-equations}

Given some [autonomous equation](#autonomous-equations), find the points where it is equal to 0. This is where \\(g(y)\\) is equal to 0. Place these points on a number line from \\(-\infty\\) to \\(\infty\\). Compute the signs for each region on this line. For each region, draw arrows pointing in the direction of signs for each region. For example, for a positive region, arrows would point right.

These arrows represent the asymptotic behavior of a solution in that region as \\(t\\) approaches \\(\infty\\). As \\(t\\) approaches \\(-\infty\\), the solution will follow the opposite direction of the arrows.

The points placed to indicate zeroes are hard boundaries. Because of [Existence and Uniqueness Theorem](#existence-and-uniqueness-theorem), nothing can cross these boundaries. Zeroes where the arrows on both sides point toward it are **stable**, while those where the arrows on both sides point away from it are **unstable**, and those where the arrows on both sides have conflicting directs are **semi-stable**.

With this information, we can draw a graph of many possible solutions. Any solution in a given \\(y\\) region of zeroes will follow the trends in the phase portrait. Infinitely many lines can be drawn from the given portrait but none of the lines can cross (that would violate the [Existence and Uniqueness Theorem](#existence-and-uniqueness-theorem)).


## Applications {#applications}


### Tank Problems {#tank-problems}

The general rule for these types of problems is:
\\[\frac{dy}{dt} = \text{Rate In} - \text{Rate Out}\\]
For problems that deal with some concentration \\(C(t)\\) of a certain mass \\(S(t)\\) in a volume \\(V(t)\\) and there is some flow \\(F(t)\\) both in and out
\\[\frac{dS}{dt} = F\_{in}C\_{in} - F\_{out}C\_{out}\\]
If the volume stays constant, \\(C\_{out}\\) is a constant, otherwise, it should be rewritten in terms of \\(S\\) and \\(t\\) by determining an equation for \\(V(t)\\) from the difference in \\(F\_{in}\\) and \\(F\_{out}\\).


## Numerical Methods {#numerical-methods}


### Euler's Method {#euler-s-method}

One can use explicit Euler's method to estimate a function \\(y\\) at \\(n\\) steps of \\(h\\) from the initial value.
\\[y\_{n+1} = y\_n + hf(t\_n,y\_n)\\]


## Exact Differential Form {#exact-differential-form}


### Formal Work {#formal-work}

We want a solution to a first order equation in the form:
\\[ \frac{dy}{dx} = f(x,y)\\]
We define some \\(H\\) to be the integral of the above equation and which defines all possible solutions \\(Y\\) to the differential equation:
\\[H(x,y) = c\\]
possibly with some initial condition \\(Y(x\_0)=y\_0\\). The question then is when can we get such an \\(H\\)?
Assume such a \\(H\\) exists and that \\(Y(x)\\) is a solution to the differential equation.
\\[H(x,y) = H(x,Y(x))\\]
Then, we take the partials with respect to both \\(x\\) and \\(y\\).
\\[\partial\_x H(x,Y(x)) + Y'(x)\partial\_y H(x,Y(x))\\]
Solving for \\(Y'(x)\\), gives us:
\\[Y'(x) = \frac{dy}{dx} = - \frac{\partial\_x H(x,Y(x))}{\partial\_y H(x,Y(x))}\\]
Since \\(y=Y(x)\\) is a solution and \\(\frac{dy}{dx} = f(x,y)\\),
\\[f(x,y) = - \frac{\partial\_x H(x,y)}{\partial\_y H(x,y}\\]
We define some \\(M(x,y) = \partial\_x H(x,y)\\) and some \\(N(x,y) = \partial\_y H(x,y)\\)
\\[f(x,y) = - \frac{M(x,y)}{N(x,y)}\\]
Remember that we need there to be some function \\(H(x,y)\\) that fits our above work. Some function \\(H(x,y)\\) exists and the differential form is **exact** if \\(\partial\_y M(x,y) = \partial\_x N(x,y)\\).
If so, we can integrate either \\(Mdx\\) or \\(Ndy\\). Doing either gives us:
\\[H(x,y) = g(x,y) + h\\]
where \\(g\\) is the integral and \\(h\\) is some function of only the other variable. We then take the derivative of this with respect to the other variable and check it with its partial of \\(H\\). With that expression, we can solve for \\(h\\), and thereby for \\(H\\).


### Informal Description {#informal-description}

Given some differential equation \\(\frac{dy}{dt} = f(x,y)\\) , we want to know when it has a solution.

It'll have a solution when we can get into some implicit form \\(H(x,y) = c\\). If we take the derivative of \\(H\\), we get some expression equal to \\(f(x,y)\\). This expression can be rewritten as \\(f(x,y) = - \frac{M(x,y)}{N(x,y)}\\) where \\(M\\) is the \\(x\\) partial of \\(H\\), and \\(N\\) is the \\(y\\) partial.

According to Euler, we can only have this hold when \\(M\_y = N\_x\\), that is, when the second order partials are the same. If they are, we can integrate \\(M\\) with respect to \\(x\\) and \\(N\\) with respect to \\(y\\) to solve for \\(H\\).


### Integrating Factor {#integrating-factor}

Not all differential equations of the above form are **exact**. However, this does not mean they are not solveable. Some equations can multiplied through by some integrating factor \\(\rho(x,y)\\). The solutions to
\\[M(x,y)dx + N(x,y)dy = 0\\]
and
\\[\rho(x,y)M(x,y)dx + \rho(x,y)N(x,y)dy = 0\\]
are the same. From there, since this must be exact, the second-order partials must equal, and so:
\\[\partial\_y[\rho(x,y)M(x,y)] = \partial\_x[\rho(x,y)N(x,y)]\\]
Applying the chain rule:
\\[\rho\_y(x,y)M(x,y) + \rho(x,y)M\_y(x,y) = \rho\_x(x,y)N(x,y) + \rho(x,y)N\_y(x,y)\\]
This is a pretty bad equation, but we can solve it if we set either partial of \\(\rho\\) to 0. \\(\rho\\) then becomes a function of only one variable, and if we can write its partial in terms of only that variable, we can find a implicit solution.

If we set \\(\rho\_y\\) to 0:

\\[\rho(x,y)M\_y(x,y) = \rho\_x(x,y)N(x,y) + \rho(x,y)N\_x(x,y)\\]
\\[\rho(x,y)M\_y(x,y) - \rho(x,y)N\_x(x,y) =  \rho\_x(x,y)N(x,y)\\]
\\[\rho(x,y) \frac{M\_y(x,y) - N\_x(x,y)}{N(x,y)} = \rho\_x(x,y)\\]

Similarly, if we set \\(\rho\_x\\) to 0:

\\[\rho\_y(x,y)M(x,y) + \rho(x,y)M\_x(x,y) = \rho(x,y)N\_y(x,y)\\]
\\[\rho\_y(x,y)M(x,y) = \rho(x,y)N\_x(x,y) - \rho(x,y)M\_y(x,y)\\]
\\[\rho\_y(x,y) = \rho(x,y) \frac{N\_x(x,y) - M\_y(x,y)}{M(x,y)}\\]

From here, you can solve for \\(\rho\\), and then solve \\(H\\) where both partial components are multiplied by \\(\rho\\).


## Higher Order Linear Differential Equations {#higher-order-linear-differential-equations}


### Definition and Terminology {#definition-and-terminology}

Every \\(n^th\\) order linear DE can be written in the form:
\\[\frac{d^ny}{dt^n}+a\_1(t)\frac{d^{n-1}y}{dt^{n-1}}+...+a\_{n-1}(t)\frac{dy}{dt}+a\_n(t)y=f(t)\\]

To get a specific solution for an \\(n^th\\) order equation, we need \\(n\\) initial conditions because repeated derivatives lose more and more information:
\\[Y(t\_1)=y\_0, Y^{(1)}(t\_1)=y\_1, ..., Y^{(n-1)}(t\_1)=y\_{n-1}\\]


#### Coefficient Functions {#coefficient-functions}

The functions \\(a\_1(t)...a\_n(t)\\) are called coefficient functions. When all of the coefficient functions do not vary with \\(t\\), the equation is said to be **constant coefficient**.


#### Forcing Functions {#forcing-functions}

The function \\(f(t)\\) is called the forcing function. When the forcing function is 0, then the equation is said to be **homogeneous**. Otherwise, it is said to be **inhomogeneous**


### Linear Differential Operator {#linear-differential-operator}

We can also define a differential operator \\(\mathbb{L}\\) that is:
\\[\mathbb{L} = D^n+D^{n-1}a\_1(t) + ... + D^1a\_{n-1}(t) + a\_n(t)\\]
where \\(D = \frac{d}{dt}\\). In essence, we factor out a \\(y\\) to make \\(\mathbb{L}\\) function-agnostic. Therefore, our initial equation can be written as:
\\[\mathbb{L}y = f(t)\\]
\\(\mathbb{L}\\) is linear, meaning applying it to a linear combination of functions is the same as applying it to each one. That is:
\\[\mathbb{L}(c\_1y\_1+c\_2y\_2) = \mathbb{L}c\_1y\_1 + \mathbb{L}c\_2y\_2\\]


### Superposition Principle {#superposition-principle}

Given some \\(n^th\\) order linear homogeneous differential equation, there are generally \\(n\\) solutions \\(Y\_1(t), Y\_2(t) ... Y\_{n-1}(t)\\). If each of these are solutions, then any linear combination of them are. This is because \\(\mathbb{L}\\) is linear.


### Wronskian {#wronskian}

We can check if some set of possible solutions \\(Y\_1(t),Y\_2(t)...Y\_{n-1}(t)\\) are independent over some interval if the Wronskian is not 0.
\\[\mathbb{W} = \det \begin{vmatrix} Y\_1(t) & Y\_2(t) & ... & Y\_n(t) \\\ Y^(1)\_1(t) & Y^(1)\_2(t) & ... & Y^(1)\_n(t) \\\ \vdots & \vdots & \ddots & \vdots \\\ Y^{(n-1)}\_1(t) & Y^{(n-1)}\_2(t) & ... & Y^{(n-1)}\_n(t)  \end{vmatrix}\\]
Anywhere \\(\mathbb{W}\\) is not 0, the solutions are independent.
